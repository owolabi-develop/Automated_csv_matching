{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Cv Macthing Phase One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pydantic import BaseModel\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "from langchain.indexes import SQLRecordManager,index\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    S3FileLoader,\n",
    "    S3DirectoryLoader,\n",
    "    Docx2txtLoader\n",
    "    )\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "os.environ['PINECONE_API_KEY'] = ''\n",
    "api_keys = ''\n",
    "\n",
    "index_name = \"job-descriptions\"\n",
    "\n",
    "embeddings =OpenAIEmbeddings(api_key=api_keys)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=6000,\n",
    "        chunk_overlap=500,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### file loader func\n",
    "## Phase 1 (one time process)\n",
    "\n",
    "\n",
    "\n",
    "def embed_load_document_to_pincone(file_path): \n",
    "    \"\"\"\n",
    "    load job description to pinecone\n",
    "    check for file extension\n",
    "    if pdf load pdf loader\n",
    "    if docx load docx loader\n",
    "    \n",
    "    \"\"\" \n",
    "    ## initializing langchain indexing\n",
    "    source = 'resume'\n",
    "    namespace = f'pincone-{source}'\n",
    "    record_manager = SQLRecordManager(namespace, db_url=f'sqlite:///resume.sql')\n",
    "    record_manager.create_schema()\n",
    "    \n",
    "    \n",
    "    ## checking file_path extension\n",
    "    file_extention = pathlib.Path(file_path).suffix\n",
    "    if file_extention == \".pdf\":\n",
    "        pdf_loader =  PyPDFLoader(file_path)\n",
    "        doc = text_splitter.split_documents(pdf_loader.load())\n",
    "    else:\n",
    "        word_loader = Docx2txtLoader(file_path)\n",
    "        doc = text_splitter.split_documents(word_loader.load())\n",
    "        \n",
    "        \n",
    "    ## indexing document Avoid writing duplicated content into the vector store\n",
    "    store = PineconeVectorStore(text_key='text',embedding=embeddings, index_name=index_name)\n",
    "    index(doc, record_manager, store, cleanup=\"incremental\", source_id_key='source')\n",
    "    \n",
    "        \n",
    "    for docs in doc : print(docs.page_content) \n",
    "    \n",
    "   \n",
    "    ##  loading document to pincone vector \n",
    "    vectostore = PineconeVectorStore.from_documents(doc, embeddings, index_name=index_name)\n",
    "    \n",
    "    \n",
    "    print(\"document uploaded to pincone db.........\")\n",
    "    \n",
    "embed_load_document_to_pincone(\"samplefile/wordJobDescription.docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Cv Macthing Phase two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### file loader extractor func\n",
    "## Phase 2 (batch and realtime)\n",
    "\n",
    "def resume_loader(storage_name=None, localfile=None):\n",
    "  \n",
    "    ## checking storage type s3 or dropbox\n",
    "    if storage_name == 's3':\n",
    "      s3_loader = S3DirectoryLoader(\n",
    "        'resume-bucket',\n",
    "        prefix=\"resume\",\n",
    "        aws_access_key_id=\"\", \n",
    "        aws_secret_access_key=\"\"\n",
    "        )\n",
    "      doc = text_splitter.split_documents(s3_loader.load())\n",
    "      return doc\n",
    "    \n",
    "    elif storage_name == 'dropbox':\n",
    "        pass\n",
    "    else:\n",
    "      \n",
    "       ## checking file_path extension\n",
    "      if localfile:\n",
    "        file_extention = pathlib.Path(localfile).suffix\n",
    "        if file_extention == \".pdf\":\n",
    "            pdf_loader =  PyPDFLoader(localfile)\n",
    "            doc = text_splitter.split_documents(pdf_loader.load())\n",
    "            \n",
    "        else:\n",
    "            word_loader = Docx2txtLoader(localfile)\n",
    "            doc = text_splitter.split_documents(word_loader.load())\n",
    "          \n",
    "        #for docs in doc : print(docs.page_content) \n",
    "        return doc\n",
    "          \n",
    "resume_loaders = resume_loader(localfile='samplefile/Owolabi_Akintan_resume.pdf')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Similarity search \n",
    "\n",
    "client = instructor.from_openai(OpenAI(api_key=api_keys))\n",
    "\n",
    "\n",
    "class MatchResume(BaseModel):\n",
    "    \"\"\"\n",
    "    instructor response_model class\n",
    "    \n",
    "    \"\"\"\n",
    "    resume: str \n",
    "    \n",
    "\n",
    "def job_description_augment_prompt_match(resume_query):\n",
    "    \n",
    "    # get top 3 results from knowledge base\n",
    "    \n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    results = vectorstore.similarity_search_with_relevance_scores(resume_query, k=3)\n",
    "    \n",
    "    ## retrive result and score and perform rag process using instructor  \n",
    "    for result, score in results:\n",
    "        \n",
    "        source_knowledge = f\"Document: {result.page_content},\\n Score: {score}\"\n",
    "        \n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            response_model=MatchResume,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a resume job description matcher. If Resume does not match the job description, return 'No Match'.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"using the Contexts: Job Description: \\n {str(source_knowledge)} \\n  match the following Resumes : {resume_query}\"}\n",
    "            ]\n",
    "        )\n",
    "     \n",
    "\n",
    "    return resp.model_dump() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webhook api\n",
    "\n",
    "\n",
    "def webhook_api(payload):\n",
    "    header = {'Content-Type':'application/json',\n",
    "              'X-Api-Key':'ef9f9fb91d819f7fae0fda9da47b11116e28564c49cd60d2017917ca99b6c3f4'}\n",
    "    url = \"https://sapphire.contextdata.ai/api/webpush/c5-000001uhj6bd/\"\n",
    "    response = requests.post(url,json=payload,headers=header)\n",
    "    return response.status_code\n",
    "\n",
    "\n",
    "## send match resume to webhook api endpoint\n",
    "for don in resume_loaders:\n",
    "    matcher_resume_payload = job_description_augment_prompt_match(don.page_content[:]) \n",
    "    print(\"uploading match resume to webhook payload\")\n",
    "    webhook_api(matcher_resume_payload)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with Your PDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    )\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "os.environ['PINECONE_API_KEY'] = ''\n",
    "api_keys = ''\n",
    "index_name = \"chat\"\n",
    "\n",
    "embeddings =OpenAIEmbeddings(api_key=api_keys)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'The type 1 diabetes story is one of the power of self-management. The tools for this are technological, but the challenge is social and psychological. We now know that structured education works, but it is more complicated and less glamorous than a new drug, and so isn’t currently given the attention that it demands. We also know that education on its own doesn’t do the job. Teaching people to do things in a new way only gives them another option. While some people with diabetes embrace the flexibility of insulin that fits into their lives, for others, this means more uncertainty and more cause for concern. As with any other aspect of life, different people have different approaches to routines, and they have wildly varying everyday circumstances.\\nDiabetes education and self-management are about more than',\n",
       " 'page': 27,\n",
       " 'source': 'samplefile\\\\pdfquery\\\\politics_talk.pdf'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "client = instructor.from_openai(OpenAI(api_key=api_keys))\n",
    "\n",
    "\n",
    "class PfdResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    instructor response_model class for pdf response\n",
    "    \n",
    "    \"\"\"\n",
    "    text: str \n",
    "    page: int\n",
    "    source:str\n",
    "    \n",
    "    \n",
    "\n",
    "def load_pdf(file_path):\n",
    "    \n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load()\n",
    "    doc = text_splitter.split_documents(pages)\n",
    "    vectostore = PineconeVectorStore.from_documents(doc, embeddings, index_name=index_name)\n",
    "    \n",
    "    return vectostore\n",
    "    \n",
    "\n",
    "def query_pdf(query):\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    results = vectorstore.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    for result, _ in results:\n",
    "        source_knowledge = f\"text: {result.page_content},\\n Page: {result.metadata['page']}, \\n Source: {result.metadata['source']}\"\n",
    "        \n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            response_model=PfdResponse,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a pdf summarizer assistant. If does not match the query description, dont generate random once just return 'No such document'.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"using the text: \\n {str(source_knowledge)} \\n  to answer the query: {query}\"}\n",
    "            ]\n",
    "        )\n",
    "     \n",
    "\n",
    "    return resp.model_dump() \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "res = query_pdf('management to enable dietary freedom in people with type')\n",
    "\n",
    "res\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
