{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Cv Macthing Phase One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pydantic import BaseModel\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "from langchain.indexes import SQLRecordManager,index\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    S3FileLoader,\n",
    "    S3DirectoryLoader,\n",
    "    Docx2txtLoader\n",
    "    )\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "os.environ['PINECONE_API_KEY'] = ''\n",
    "api_keys = ''\n",
    "\n",
    "index_name = \"job-descriptions\"\n",
    "\n",
    "embeddings =OpenAIEmbeddings(api_key=api_keys)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=6000,\n",
    "        chunk_overlap=500,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### file loader func\n",
    "## Phase 1 (one time process)\n",
    "\n",
    "\n",
    "\n",
    "def embed_load_document_to_pincone(file_path): \n",
    "    \"\"\"\n",
    "    load job description to pinecone\n",
    "    check for file extension\n",
    "    if pdf load pdf loader\n",
    "    if docx load docx loader\n",
    "    \n",
    "    \"\"\" \n",
    "    ## initializing langchain indexing\n",
    "    source = 'resume'\n",
    "    namespace = f'pincone-{source}'\n",
    "    record_manager = SQLRecordManager(namespace, db_url=f'sqlite:///resume.sql')\n",
    "    record_manager.create_schema()\n",
    "    \n",
    "    \n",
    "    ## checking file_path extension\n",
    "    file_extention = pathlib.Path(file_path).suffix\n",
    "    if file_extention == \".pdf\":\n",
    "        pdf_loader =  PyPDFLoader(file_path)\n",
    "        doc = text_splitter.split_documents(pdf_loader.load())\n",
    "    else:\n",
    "        word_loader = Docx2txtLoader(file_path)\n",
    "        doc = text_splitter.split_documents(word_loader.load())\n",
    "        \n",
    "        \n",
    "    ## indexing document Avoid writing duplicated content into the vector store\n",
    "    store = PineconeVectorStore(text_key='text',embedding=embeddings, index_name=index_name)\n",
    "    index(doc, record_manager, store, cleanup=\"incremental\", source_id_key='source')\n",
    "    \n",
    "        \n",
    "    for docs in doc : print(docs.page_content) \n",
    "    \n",
    "   \n",
    "    ##  loading document to pincone vector \n",
    "    vectostore = PineconeVectorStore.from_documents(doc, embeddings, index_name=index_name)\n",
    "    \n",
    "    \n",
    "    print(\"document uploaded to pincone db.........\")\n",
    "    \n",
    "embed_load_document_to_pincone(\"samplefile/wordJobDescription.docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Cv Macthing Phase two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### file loader extractor func\n",
    "## Phase 2 (batch and realtime)\n",
    "\n",
    "def resume_loader(storage_name=None, localfile=None):\n",
    "  \n",
    "    ## checking storage type s3 or dropbox\n",
    "    if storage_name == 's3':\n",
    "      s3_loader = S3DirectoryLoader(\n",
    "        'resume-bucket',\n",
    "        prefix=\"resume\",\n",
    "        aws_access_key_id=\"\", \n",
    "        aws_secret_access_key=\"\"\n",
    "        )\n",
    "      doc = text_splitter.split_documents(s3_loader.load())\n",
    "      return doc\n",
    "    \n",
    "    elif storage_name == 'dropbox':\n",
    "        pass\n",
    "    else:\n",
    "      \n",
    "       ## checking file_path extension\n",
    "      if localfile:\n",
    "        file_extention = pathlib.Path(localfile).suffix\n",
    "        if file_extention == \".pdf\":\n",
    "            pdf_loader =  PyPDFLoader(localfile)\n",
    "            doc = text_splitter.split_documents(pdf_loader.load())\n",
    "            \n",
    "        else:\n",
    "            word_loader = Docx2txtLoader(localfile)\n",
    "            doc = text_splitter.split_documents(word_loader.load())\n",
    "          \n",
    "        #for docs in doc : print(docs.page_content) \n",
    "        return doc\n",
    "          \n",
    "resume_loaders = resume_loader(localfile='samplefile/Owolabi_Akintan_resume.pdf')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Similarity search \n",
    "\n",
    "client = instructor.from_openai(OpenAI(api_key=api_keys))\n",
    "\n",
    "\n",
    "class MatchResume(BaseModel):\n",
    "    \"\"\"\n",
    "    instructor response_model class\n",
    "    \n",
    "    \"\"\"\n",
    "    resume: str \n",
    "    \n",
    "\n",
    "def job_description_augment_prompt_match(resume_query):\n",
    "    \n",
    "    # get top 3 results from knowledge base\n",
    "    \n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    results = vectorstore.similarity_search_with_relevance_scores(resume_query, k=3)\n",
    "    \n",
    "    ## retrive result and score and perform rag process using instructor  \n",
    "    for result, score in results:\n",
    "        \n",
    "        source_knowledge = f\"Document: {result.page_content},\\n Score: {score}\"\n",
    "        \n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            response_model=MatchResume,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a resume job description matcher. If Resume does not match the job description, return 'No Match'.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"using the Contexts: Job Description: \\n {str(source_knowledge)} \\n  match the following Resumes : {resume_query}\"}\n",
    "            ]\n",
    "        )\n",
    "     \n",
    "\n",
    "    return resp.model_dump() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading match resume to webhook payload\n",
      "uploading match resume to webhook payload\n"
     ]
    }
   ],
   "source": [
    "# Webhook api\n",
    "\n",
    "\n",
    "def webhook_api(payload):\n",
    "    header = {'Content-Type':'application/json',\n",
    "              'X-Api-Key':'ef9f9fb91d819f7fae0fda9da47b11116e28564c49cd60d2017917ca99b6c3f4'}\n",
    "    url = \"https://sapphire.contextdata.ai/api/webpush/c5-000001uhj6bd/\"\n",
    "    response = requests.post(url,json=payload,headers=header)\n",
    "    return response.status_code\n",
    "\n",
    "\n",
    "## send match resume to webhook api endpoint\n",
    "for don in resume_loaders:\n",
    "    matcher_resume_payload = job_description_augment_prompt_match(don.page_content[:]) \n",
    "    print(\"uploading match resume to webhook payload\")\n",
    "    webhook_api(matcher_resume_payload)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
